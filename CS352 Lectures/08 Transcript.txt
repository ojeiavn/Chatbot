SPEAKER 0
That's the one.

SPEAKER 1
So. So what I'll do is I'll probably speaking to about just after 4:00 am, a five minute break. I'll give you ten minutes over the microphone. Okay. Is that that. Yeah. The other thing is about 40 minutes, I think. So it should be. I should work up quite nicely. Okay. Yeah, yeah. Feel free to log into the app and answer the questions and things and get stuck in if you know. Maybe record numbers. We'll have to wait and see. I should say that the camera the camera captures from here to about here. See what can stand in this area or behind the desk, depending on where you find this in. It's good to see everyone embracing the just in time philosophy of lean that we've learned about. That's nice to see you arriving just in the nick. That's perfect. Right on the second. Well done. So today we're talking about managing risk. There are a lot of risks in project management, not least the risk of reaching down and getting the PMBoK guide off a high shelf. Often happens to me that it falls on my head. So we're going to start by talking a bit about keeping safe. These guys in World War One are wearing this really clever innovation called a helmet. It was invented during World War One. And interestingly, the introduction of the helmet actually caused an increase in head injuries. Why did the helmet introduction during World War one increase head injuries? Anyone want to volunteer a guess as to why that might be? Sorry, I didn't get that. Sorry. I see. So it made people behave more recklessly because they felt safe with the helmet on. Nice theory. Any other guesses? Yeah.

SPEAKER 2
Like.

SPEAKER 1
Yeah, that's that's the correct answer. So I think both answers are really good, but the correct answer is that people were dying before and now they were just coming away with with slightly less major injuries because of the helmet. The reason I introduced this is because the statistics on risk are very difficult to interpret. Sometimes it's difficult to understand what these things mean. But let's keep on this on this idea of helmet related statistics. If you look at causes of head injuries, you'll notice that car drivers are far more likely to suffer serious head injuries, followed by people just maybe hiking or doing leisure activities, followed by motorcyclists, followed by people doing work related activities, followed by those who are victims of crime, followed then by cycling, and finally by people who are pedestrians. So it's strange that pedestrians are the ones who are expected to wear helmets and none of these other people. Obviously you couldn't be expected to wear a helmet in your home just in case you got broken into, even though the risk is greater to you in your own home of head injury due to a burglar than it is if you go out on your bike to work. Which is strange, it doesn't seem to add up. The numbers are very confusing, right? Here are some facts about cycle helmets. I'm not going to express an opinion one way or the other, but these facts have been selected due to my own personal bias, so please bear that in mind. Bike helmets actually may reduce may reduce the risk of head injury by up to 88%. So wearing that helmet and flinging yourself into a wall, you're far more likely to get head injury without the helmet than if you were wearing the helmet. That seems pretty obvious. That's why the helmets were designed the way they were, so that seems like a good reason to wear a helmet. However, motor vehicle occupants are more likely to suffer head injuries than cyclists, so maybe the drivers should be wearing the helmets, not the cyclists. It's important to note that the health benefits of cycling outweigh the life. Years lost by a factor of 20 to 1. That means your life is longer by cycling without a helmet. More than it would be by wearing a helmet and not cycling. If that makes sense. That's a bizarre way of putting it. The health benefits far outweigh the risks. That's the key point, right? The risks of cycling are very, very low. The chance of being in an accident that involves a head injury are very, very low, which is why a helmet has probably almost negligible effect on your overall health. I was I was, however, in a cycle accident just last week. My helmet did help me. So that's not to say those very unlikely events don't happen. They do happen. Um, interesting observation we had in the middle there. Which which is that wearing a helmet changes how drivers perceive the cyclist and similarly changes how cyclists behave. So cyclists may behave more recklessly. Drivers may behave more recklessly when they see a helmet. They think, you know what you're doing, or you think you know what you're doing because of the helmet, and it makes you behave more dangerously. It's known that drivers drive closer to you if you're wearing a helmet or if you look like you're a good cyclist, whereas if you're swerving around everywhere, they tend to give you a wide berth. So maybe that's some good advice. Maybe you should swerve a lot when you cycle, or maybe not. I'm. So the reason this is interesting because in Australia there was a helmet law introduced. And this caused a decrease in the popularity of cycling. It reduced the number of people cycling, which in the long term has a much worse effect on people's health. By not cycling, they're going to die younger because they're less healthy. So the actual damage to people's health in the long term is actually far worse. By making people think cycling is dangerous. So it caused. So the stat there is a 44% decrease in children's cycling. Which is five times bigger than the number of children who started wearing helmets. So it creates this perception of danger, which perhaps is not grounded in reality. And that's one of the dangers. So risk risk is a very strange thing. People, human beings, are very, very odd when it comes to risk. We're not good at dealing with risk. It's not something we can understand intuitively, and it's something that causes all sorts of confusion. So today we're going to talk about risk in projects and hopefully try and ground it in some reality in some facts, so that you can make risk aware decisions that are reasoned, well considered and not based on strong bias. Okay. So this is the definition of risk. And it has changed the industry standard definition of risk has changed. It used to be the chance of probability the chance or probability of loss. The definition of risk is now the effect of uncertainty on objectives. So it's not it's no longer emphasising loss. Risk doesn't have to be a negative thing I think that's important to stress. Risk. We take risks intentionally because there is a potential opportunity. And that's something that the change in terminology in the industry standard definition of risk captures. So how do we deal with risk? Well, we could avoid the risk. That's a that's a good way of doing it. We can perhaps discontinue the risky activity, but we probably wouldn't have much of a life if we just discontinued every risky activity. We could remove the source of the risk. We could try and change. Change the situation in a project, you might be able to change things around to remove the risk or remove the source of the risk. Or try manipulate the likelihood. Do things differently to change the probability of it happening, or change the consequences by protecting yourself, perhaps by wearing a helmet or in a project, perhaps doing things that are equivalent. You can share the risk, so you can perhaps buy insurance. You can retain the risk. You can decide, okay, this risk is inevitable. We have to accept it. Um, or you can actually increase the risk because it may be a positive risk. As I already said, a risk is not necessarily negative. Let's start with a bit of a video. It's auto playing. That's good. Keeping with the cycling theme, we don't really sound effect. The music's good on this one. Someone's obviously muted, the muted the music. I won't beatbox, you'll just have to imagine there's some cool music in the background. I think this might actually be staged. Unfortunately, I don't think this was in a real competition. The real risk taker, I think, is the motorcyclist, who takes both hands off their handlebars to take a photo. That is the real idiot in this. But this guy taking a serious risk with his reproductive health, potentially by doing that, but managed to get ahead of all the others by taking a more ergonomic approach to cycling. Okay. The reason I show that is to really emphasise that you take risks for a reason. You don't take risks pointlessly. You take risks because there is a significant opportunity, a significant opportunity to to gain something that is worth taking. It's there's no point in taking risks that aren't worth taking. Right? You avoid those ones, you mitigate those ones. But there needs to be a positive angle to risk. It's important to think of the positives. So risk the effect of uncertainty on objectives. Let's define some of those terms. To be clear. Uncertainty. Uncertainty is something that is not certain right. It's something that's not definite. It may or may not happen. It has a probability associated with it. There are different sources of uncertainty. They could be the events that may or may not happen. It could be just that we don't have information. Uncertainty comes from lack of knowledge. Lack of information. Your third year project started with a lot of uncertainty. Hopefully that uncertainty is as reduced to the point of your progress report now, which is due in next week, and hopefully the uncertainty will be gone. At your progress report, we'll have a very clear plan to the next term's work. We have uncertainty in time. We have uncertainty in cost. You may have some uncertainty in terms of the quality of what you're going to make. Health and safety is a source of uncertainty. Legalities are a source of uncertainty if legal landscapes change. But risk is not some kind of fixed quantity, right? People have different appetites for risk, different tolerances. So a person may be risk averse or a person may be prone to taking risk. We all have different appetites when it comes to risk taking. And the organisation itself may have different attitude towards risk, depending on if it has a lot of spare cash and is prepared to take those risks to gain the benefits of the opportunity, or if it's very careful, with its money. So it isn't going to expose itself to too many risks. The threats versus opportunities. This point I've already made multiple times now. Risk is not necessarily negative. There may be benefits that are intended or even unexpected. And known risks are the ones that we've identified. We try to identify risks at the start of a project because we need to manage risk, because we may not have an infinite budget to deal with unexpected things. And those known risks are under the control of the project manager, and they tend to be paid for out of the contingency reserve we talked about during the budgeting week. Right. They are things that we have accounted for. They are within our tolerance. There are things that we expect and can therefore deal with when they arise. But of course, unknown things happen to you. Can't see the future. Things happen unknown. These are usually the responsibility of the senior management. If these risks occur, you need to tell. Tell the board. You need to tell the boss. You need to escalate these things and the money will come out of a different reserve. Perhaps the management reserve. Now risk is different to an issue. An issue is what happens when a risk comes to life, when it happens, when that risk occurs, it is now called an issue. Okay. So let's let's see who we've got in the room here. I'm guessing since you're all at a lecture, you're probably all very risk averse people. You don't want to miss out on any really important nuggets of content. But there are people at home, of course, so maybe they've got slightly different risk appetites. So what I'm going to do imagine this I'm going to toss a coin. Let's see if I've got a coin I do I've got a coin this week okay I've got a coin I'm going to toss this right. I want you to choose a gamble. When I toss this coin I want you to choose heads. You don't choose heads or tails. You're going to choose gamble one. Gamble zero to gamble five. So you've got a choice of all these gambles. Now, if you choose gamble five, you will only get paid £2,000 if it's tails. Otherwise if it's heads, you get nothing right? Whereas if you want to be a bit less risky, you might take this option. Here. You'll get £100 if it's heads, but only 1900 if it's tails, or if you're kind of being a bit more cautious, you can get £500, whether it's heads or tails at the bottom there. So I want you to pick a gamble and tell me what you think you would do. So study these carefully and I'll take your answers in the poll. Okay, I'll stop the poll there. It seems like most people have answered 21 out of 21. That seems okay. Okay, so an interesting spread here, right. So one of you has gone for this very, very risk averse strategy down the bottom here. You just want £500. You're not prepared to gamble anything. You weren't even even prepared to gamble £50 of that for the potential £450 gain. You know, I'm going to toss that coin. Let's find out. Just a disclaimer. I'm not actually going to pay. It's a. Ted. So well done to gamble zero. You got lucky this time. You got the most. But that's a very risk averse strategy because the payout the differential payout is much, much higher. If you just sacrificed that £50. And this continues right. For a £50 loss on heads, there's a potential gain there. So you can actually calculate obviously the expected the expected gain from each scenario, the minimum expected reward, the expected payout was only £500 in this scenario. Whereas in this very risky scenario at the top or the more risky scenario, the expected payout is the highest. So bizarrely, this is risk neutral at the top. You might have felt like it was a risk prone thing to do, but actually as risk neutral to go for the maximum expected return. Um. And as you go down the list, you become increasingly risk averse. So it's quite interesting to see where you where you lie. Just sort of have a look at that then compared to your results. The majority of you were kind of on the risk neutral side, but a large chunk of you then went for gamble three, which is kind of moderate risk aversion. Um, and there are reasons for this, right? It's because, you know, our our perception of value of money is not linear. Right. If I said to you, you could either win £100 or nothing, you'd be like, oh, I'd like to win £100. But if I said you could win £1,900 or 2000, you probably wouldn't mind. But they're both big numbers and you're happy with either, whereas zero versus 100 feels bigger, right? Would you, you know, drive for miles to get £100 off a laptop when the laptop costs, you know, thousands of pounds? Possibly not. Right. It might not be worth your time, but would you pay £100 for a bus into Warwick? No, of course not. Right. It doesn't matter, right? It's our perception of risk is a very strange and non-linear thing. So because risk is such a different thing to different organisations, and because we have different appetites, there are different approaches to risk management in the PMI approach and the guide, you'll see there's quite a systematic breakdown of knowledge areas and process groups. That's because there is a risk of missing something. So if you're taking the PMI approach they cover all their bases and give you all the best practices and allow you, as a project manager to pick which one you think is best. There's a risk knowledge area, of course, where you can explicitly plan to avoid risk. So risk is covered as a knowledge area in its own right. So that obviously the PMBoK guide gives a nice broad view of risk and different approaches to risk management. So I guess it covers covers all bases or all opportunities. Prince two is a lot more specific as a methodology. So you've got a board of directors, a board of directors that sort of make all those cutthroat decisions and avoid the risk of losing control on the project. That comes from the fact that Princeton is a very risk averse scenario. It's very business oriented. It cares about the business case and wants to make sure that the business case is always, always managed. So to avoid that risk of losing control, it appoints that board of directors. The board members have different roles, right? Because you've got a senior user, you've got an executive, you've got senior suppliers. And that's because stakeholders are important. And there's a risk that some stakeholders won't be satisfied if they're not all represented. That's why the board is designed as it is. And of course, the business case theme avoids the risk of failing to meet business needs. So Prince two is designed around the risks it is designed to deal with. You know, these methodological details come from its appetite to risk. Now, you might think that agile and lean are not really risk management methodologies, but of course they are. Agile is about the risk of failing to meet the customer needs buy, and it deals with that by putting the customer in the loop and having iterations. It was it's dealing with that very known and very common risk in software development that things don't get delivered to plan. Um. And Leon deals with the fact that there is a risk of overproduction or overburden or causing inefficiencies. And therefore it introduces these concepts of minimising waste. And similarly, through the Minimum Viable product concept, it maximises value to the customer that minimises the risk of delivering something that isn't valuable to them. So all methodologies that you use will be defined around the risks that are being dealt with. So let's have a quick look at the PMBoK guide and see where we are for risk. There it is. There's the risk knowledge area. And we've got all these different risk related things that we can do. Um. So most methodologies will have a cycle of planning and implementation to deal with risk. This is all relatively simple stuff. You first of all identify the risks. Then you assess their probability and consequence. You plan strategies for dealing with those situations, and then you need to monitor the factors that could lead to those risks and finally communicate people when risks actually do happen. And you do this in an iterative way. That's why it's kind of a cycle. So you keep on doing this continuously because situations change. Projects aren't static. So risk is dealt with in projects in many different ways. Let's talk specifically about software development risk. So if you survey the literature on what the risks are in technology development, you find whole loads of different things that go wrong in projects. So I've kind of grouped these into different categories, things related to to the users of the system, things related to the development team, things related to the organisation, risks relating to planning and control to system requirements and complexity. And of course, each of these can be dealt with in different ways. So the risk of system requirements being wrong, well, we can use agile. We can get the customer in the loop and make sure that the system requirements don't need to be defined upfront, because that is a known risk. I'm the risk of complexity. Well, yes, of course, there's always a temptation to design some overly generic or complex solution because it's fun to write that. So therefore, iterative value driven solutions are good. Where there are problems with. Users or the team. You can use agile methodologies or lean methodologies. These deal with empowering the users and empowering the teams. Um. If you have issues of. I'm not sure what that relates to, does it? Is that relevant here? Nope, not sure what. That's how that fits in. Risks to do with organisational environment. Organisations have perhaps a need for control, a need for bureaucracy to manage certain things that are out of their control. And in those situations where it's a large organisation, things like Prince two and governance become dominant in methodologies, right? If you document everything with Prince two, there is an unlikely risk of someone leaving their job and not passing on the right information to their to their new to the new person who takes their place. It deals with instability, right? You may decide that that's not worth the risk, and documentation is such a burden, you'd rather use agile. But of course, there's a reason why we have that. Um, and also in terms of planning and control, right. If you need control, if you need things to fit a deadline, if you need budgets to fit within a tight budget, then you can take control. You can have a board of directors. You can make sure that you have fixed methodologies and fixed milestones. So depending on the situation, you need to choose the right methodology and the right tools. Um, so I've got a few graphs here which aren't really real graphs. They lack axes and things, but they are commonly used graphs in business literature. And I thought I might talk a little bit through them just to show you that risk is everywhere. It's not just in the development of software, but it's in the entire ecosystem of an organisation. So when you have a new idea, a new innovative idea, like an invention, you want that idea to disrupt an industry. You want to change the industry by getting people to adopt your new idea. But that's not trivial. You first of all need to get funding. You need to get money to back that idea, to develop that idea, to get it affordable enough to deploy on a mass scale. So this graph is often shown as what it's like when you have a new idea and you try to get it out there. First of all, you have this kind of this big spike of getting, you know, getting people excited. The early adopters and the tech geeks get really excited about the idea, and there's all that kind of excitement, but that novelty wears off where nothing materialises immediately, and you kind of have to survive on small amount of funding to keep things going. And sometimes things all go wrong. You make a mistake, you mess up and the business is bust, but you know you have a few kind of wiggles as things start to improve, as you start to roll things out, and eventually people start to to buy your product. Right? But that's not all. That's not all. That's not everything. This is just, you know, this is perhaps a curve of a new Start-Up. This might be their finances and this might be time over one year, for example. But after that you need to get into the mass market. And that's where this there's another practically access list curve comes in. Actually this one's got expectations on that axis. I'm not sure that's being measured, but you've got to then get it into the mass market. So you know you might have made a product that works now. Well done. But so that it's not a white elephant, you actually have to get that into a market. And that means you need to build hype. You need to build demand where it wasn't already before. You're trying to disrupt an industry, so you need to create this peak of expectations. You almost oversell. And that's what gets people to buy your product. And quite often that doesn't materialise and people don't buy it, or you fail to fulfil what you promised and people decide they don't like it. But if you can survive this trough of disillusionment and get into some kind of productivity, then in theory you've got a business that works. But of course, no you don't, because things also break. Even if they're long term established businesses, things just sometimes break because the world changes and most of these organisations are built for a particular world, a particular time. And when that world changes, their business model crashes. And they if they aren't lean and adaptable to change, then they're not going to be able to move with the times. So risk is everywhere. Risk is at all stages of a business, not just for Start-Ups or growing emerging businesses, but for all businesses. And you could argue that it's becoming ever an ever bigger problem because technology is changing at an increasing pace. It becomes a necessity to innovate at all stages of an organisation, not just at the beginning. So risk is everywhere. I think that's what I've established so far in kind of around the, around the houses kind of way. So in this lecture, then I'm going to try and go through a range of different methodologies at different tools and techniques you can use to try and manage some of those risks specific to software projects. So the first thing we're going to talk about is RACI. This is not as RACI as it sounds. It is just a matrix of responsibility and accountability, consulted and informed. You're trying to allocate people into boxes and decide where they fit in the project. Um, so, yeah, it's all about assigning responsibility. Now there are some rules when you're doing RACI matrix development that try to make sure that you don't fall into some common errors. So what you should always do is make sure that every task in your project has at least one person responsible. Um. That's the person who's actually going to do the work, right? So that's why they're responsible for it. Responsibility differs from accountability, and accountable person is the person who's blamed if it goes wrong. The accountable person is the person whose head is on the chopping block. Often, the person who's accountable is also responsible because it makes sense to have those things together. But it's good to have someone senior and important accountable, rather than just making lots of unimportant people accountable and responsible because they could just be sacked. And then who holds the you know, who's going to hold the torch then? Right? So every task should also have one and only one person accountable. If there's more than one person accountable, then they might just say, well, they should have done it right. So there can be no finger pointing. The person accountable on the RACI matrix is the one and only person accountable. It's also good to have just one person responsible as well, but it's not strictly necessary. So let's have a look at a RACI matrix and see if we think it's a good one. Can anyone spot any immediate problems here? I'll give you the quick answer there for those who are paying attention. Any issues with this one? Okay. Anyone's going to put a hand up. Yeah. Now. Then, as anyone spotted. What's wrong with this one? Based on what I just said. Yes. It's not well distributed. No, you're right, it isn't. It's not well distributed particularly, is there anything in particular that you think a badly distributed task or. Yeah, yeah. Things with no responsibilities at all as a problem. Yeah. Good. Any other problems? Anyone else? Don't be shy. It's okay. Just agreed with ours and eyes and sees on. Yeah. Sorry. What's wrong with those ones?

SPEAKER 2
It is not defined. They are. And.

SPEAKER 1
So row four here. This one. Right. A column. Sub teams. Here. These ones. Oh, yes. Okay. I see that those people those roll for, not roll for. Sorry I misheard you that. Yeah. Roll for and roll five. These people aren't assigned any information. That's okay. Not everybody needs to be assigned to to a to a task during a RACI matrix. But I mean, the point of distributing the workload is a good one. It's important to distribute things. So let me let me point through a few. So these ones here all have multiple people accountable. As I already said, one and only one person should be accountable for a given task, right? Because otherwise they'll just point the blame and say they should have done it. So there's a mistake. So you shouldn't have this this situation shouldn't be arising. Okay. Um, this isn't a good one either, right? Too many people being consulted. Let me explain what consulting means. Consulting means you ask someone their opinion, you listen to their opinion. You act on their opinion. Right? You don't have to do exactly what they say, but you involve them in the discussion. That difference from informing someone? Informing someone, you just tell them what you're going to do. Right. So consulting 123456, eight different people, that might be very problematic. Now this of course is the creation of the project charter. Maybe that is sensible to consult all of those people, but do you need to consult every single person in the team? Possibly not. Right. So this particular example, you need to think about it on a case by case basis. But all of these CS does seem like it might be an early warning flag that you're consulting too many people. It would slow the project down. I'm. You're not informing anyone in the team about these additional plans. That could be a problem. You should probably inform your team if they're going to have to do something different. On this one here. Almost everyone on the team is responsible. Now that could be an agile team and there's a collective responsibility there. So that could be okay. But if lots of people are told it's your responsibility to do this work, then who's going to do it? They might start competing saying, well, I want to do it because I want the glory. There might be a turf war on that on that basis. So it's really good to try and minimise the number of people you're making responsible and accountable. So that's something else to consider. As was already pointed out, distribution is not great. In particular, the project manager here is responsible and accountable for almost everything. That is a big mistake. You don't want to put too much burden on one person. Makes it very hard for the project manager, who probably produced this diagram to delegate and manage the work. If they're responsible and accountable for everything that happens. If that one person is ill, then there's a problem. But really, I think the board should take on more responsibility. The project leadership should really there should be a lot more accountability on the board because the board ultimately should be accountable for a project. Prince two makes that a requirement in this version of the rating matrix. They haven't. So certainly you need a lot more accountability on the board. Okay, so producing a RACI matrix is something you'll do in the seminars. It's something that that's a very useful tool for making sure you allocate responsibility and can visually see where there are emergent problems. It's also the kind of thing that comes up quite often on exam papers, so it's worth making sure you're happy with it. Okay, so once you know who's responsible for the problems, you can point the finger of blame. And that's one important part of risk management making sure that the right person is blamed and sacked when things go wrong and it's not you. And importantly for a project manager, that's that's one way a project manager gets things done by making sure that things are accountable and responsible. If someone's accountable and responsible, they can say, look, well, you agreed that you would do this. You've signed this off. You have to do it. They can't just sort of not do their work. If their name is on there, then they'll be shamed and and booed if they don't do their work. So it's a good way of getting people to do what they've said they were going to do. So once you know who's doing what. It might be worth identifying what the risks are. And this is kind of a bit of a brainstorming task. So you may have heard of some of these different methods. You've got Swat do a Swat diagram where you work out the strengths, weaknesses, opportunities and threats for a given situation. I imagine you've probably come across that before, so I won't go into detail, but if you don't know about that, Google it. It's a pretty simple idea. You can do a risk breakdown structure very similar to a work breakdown structure. Break things down hierarchically to figure out what the different risks are and the risks within those identified areas are, and keep going until you've got a thorough idea of the risks. So these are very systematic ways of trying to break down the risk landscape. If you've heard of the Delphi technique, this is a way of speaking with experts. You get those experts to anonymously and independently give their opinion. It's kind of like having two separate juries or, you know, two independent opinions from two different doctors. It's a very good way of getting a robust response from experts. And you do that repeatedly. So you iterate until a consensus is reached. But you you go through this external independent phase just to minimise bias because you don't want sort of an artificial consensus, you want a genuine consensus. And finally a method called decision tree analysis, which we'll talk about soon, I think. Which is where you basically model decisions and the probabilities of things happening, and work out the ultimate value of different decision paths that you can take. Well here's a diagram of a Swot analysis. So you split things up into. You can sort of divide them into the internal and external helpful and harmful which are the strengths, weaknesses, opportunities and threats respectively hence Swot. And I won't describe this slide because I think it's relatively straightforward. But you just break things down into those different categories to identify where the risks are going to come from. This is what a risk breakdown structure would look like. Probably looks familiar to you from previous stuff we've talked about. And this is a decision tree. So for example, in this decision tree you're asking the question should we build a prototype or not? Building a prototype has some cost incurred. It may be costing £100,000, but no prototype would be free. Right. So let's not do the prototype. We'll save money. Well, if we build a prototype, the chance of success is 70%. But it's only 20% if we don't build a prototype. The prototype gives you valuable feedback that allows you to work out what's going to happen. Now, you might be wondering where these numbers come from. That's the magic of this. You need to figure that out. You might need to guess, which is probably a pretty weak method. Or you might need to use a more robust method. You could perhaps look at previous projects, or do a pilot study or an experiment to find out what these probabilities are likely to be. So let's assume these probabilities are true. For now. The chances are much higher of success, and the payoff in a successful scenario is half £1 million. So what you can do is therefore calculate the expected monetary value. That's this number at the end, the EMV of £400,000 based on the cost of the prototype and the success. And you basically multiply that number by 70%, subtract that to get this. Also factoring in the cost of failure as well. So you just add up the numbers and work out that making the prototype is worth £235,000 to you, whereas not making the prototype is costly. Based on the the probability of loss. Does that make sense? I think we're going to walk through an example now and a more specific example. So let's say you've got a problem. You've got a buggy system. Your customers are unhappy. What should you do. So the first thing we do is we we draw a decision node that's a square. That's just the standard notation. It's probably worth memorising that so you can get it right in the exam. Three different options. We can revamp the existing system we can fix. Sorry, we can keep using the existing system.

SPEAKER 3
Which. Will.

SPEAKER 1
What's that? Oh, yeah. Okay. It'll cost us £3 million to do that. To carry on using and maintaining the existing system. We can fix and revamp the existing system, which will cost us more than that. It'll cost us 12 million. Or we can rewrite the whole system from scratch, which will cost us 25 million. Right? So these are kind of the costs of these different decisions. And then we have a chance node where we need to work out the probabilities of the outcomes that result from these choices. So we do some modelling and some focus groups with our with our customers, and figure out that keeping the existing system will have two possible outcomes. One is that we'll just be able to carry on as we are, or we'll lose all our customers. Um. If we fix the system, we could keep our customers and make 35 million. Or some bugs may remain, but some bugs may remain. And we may or may only make 15 million, because those bugs may lose some of those customers. Or if we rewrite the system from scratch, we could have a massive success as a result. But there's a risk in doing this that we'll have a huge screw up, will not deliver the project on time. The development team will all go off sick, and we'll have a screw up and we'll be in trouble, okay? And we'll have no money earned. So this is how much money we're going to earn. In contrast to this, which is how much money we were spending. So we can calculate the expected monetary value of each of these different outcomes. This is just. This. Take away this right 20. Take away three. Zero. Take away three. Okay. Simple to calculate. And we, as I already said, I thought this would have popped up earlier. These are based on our focus groups or whatever experimental method. These are the probabilities of these different outcomes happening. So using whatever method we have at our disposal, we figure out what the chances are. And when we do that, we can calculate the expected monetary value at this point by propagating the 17 million and the 90% here in this term, and the 3 million and the 10% into this term. Add the numbers up and we get. 15 million is the expected monetary value of this decision, and likewise the expected monetary value is 17 million and there is 15 million. So the most profitable decision is this one. And the way we've managed risk here is we've managed it by estimating the probabilities of different outcomes and picking the outcome with the greatest probability. Does that make sense? This is a this is standard exam stuff. So being able to do this method is something you'll almost certainly need to do in the exam. We'll get to practice this in one of the seminars. Not the first day coming but the one after I think. Okay, so that's the decision. You pick the decision with the greatest monetary value. Now in an exam situation, it might ask you which of the decisions to take. You could just say pick this one because it's got the greatest EMV. But you may also consider other factors, right. Cost is not the only factor in a project. There may be other things to consider, other aspects of risk that aren't captured here. Can anyone think of? Additional risks that haven't been considered in this in this scenario. Yeah. How much time you can afford. Yeah, exactly. That's a good one. Any of this. Yeah, again. Environmental risk. Yep. Sounds good. There are many risks. There are many risks you can consider. There may be a cash flow risk, right. If you take take on this bottom choice here. Let's say if this had a 20 million EMV picking, it requires you to find 25 million upfront to pay your developers. Your firm may not be able to afford that right, or you may not be able to tolerate this very low probability, but certainly possible outcome of the huge screw up. You know your business does not want to screw up. It would rather minimise that risk because it needs to continue business as usual. Your project is not done in isolation, it's done within a wider environment, which is why the point of environmental risk comes in. There may be other factors to consider. Okay, so we've identified the risks. Using various brainstorming methods. We tried to do that in a systematic way so we don't miss anything. That's why we had those different techniques. And once you have an idea of the risks, it's worth thinking about what caused those risks. Working backwards from those risks to figure out what caused them. Because that's really useful. Because if you want to manage something, you can't manage everything. You've got to be selective. So we have a method called Ishikawa, which is a. It probably means fishbone in some language. I don't know, it sounds Japanese. I'm no, I'm not an expert. Unfortunately, I only know three words in Japanese, which I taught you all a couple of weeks ago. But in Ishikawa you work backwards from the risk. So you start from a risk and you work backwards. Thinking about you brainstorm what the possible causes could be. You try to do that in a really systematic way, and one way to achieve that systematic ness is to use an acronym like the the six M's or the four P's. So the six M's are machines, methods, materials, measurements, mother nature and manpower. I guess this was designed for manufacturing and therefore they're perhaps biased towards that. But ideally these these six things, these six areas would cover all possible areas of risk in any given project. There are also four PS, which I think is probably the more generic method. Policies, procedures, people and plant. Plant being technology and tools that you're using. You know, the physical infrastructure perhaps. So this is perhaps a more general way of doing it. You can break it down into whatever way you think is suitable, as long as you have an exhaustive coverage of all the possible risks. So once you've decided those for which way you're going to break it down, you think of all the risks and then you work out the possible causes. So you tend to draw it like this with the outcome that you're trying to explain and the possible areas of risk like this. And then. You identify all the risks within the policies area, all the risks in the procedures area, all the risks in plant and all the risks within people. Pretty simple stuff. And once you've identified the possible risks, you can identify all the possible causes of that. So it's kind of working backwards from the problem is a very good way of systematically figuring out what it is. This is very similar to a risk breakdown structure. It's a hierarchical decomposition. But in this case it's working backwards towards the causes. It goes one step beyond the RBS. So here's an example of a diagram. I'll not talk through it, but you can study these slides in your own time to make sure you're familiar with how this method works. Here's a more complex one using a different range of sections to consider. Um. Okay. There's another method that we can use to evaluate cause and effect, and it's called sensitivity analysis. Sensitivity analysis is a bit like sort of a science experiment. You hold all variables constant and wiggle one thing and look at the effect it has. You do that for all the range of variables that are in consideration, to establish which variables you're more sensitive to as an organisation and which ones you're less sensitive to. I'm. So. The purpose of sensitivity analysis, then, is to determine how different values of an independent variable impact a particular dependent variable under a given set of assumptions. That's the definition there. Um, so how do you come up with the numbers? Well, you could do a real experiment. You could gather some real empirical data and therefore try and correlate the risk factors with the different outcomes. You can actually change the price and see if your customers change their mind and buy your product or or not. In most scenarios, you can't actually start changing your business and seeing what happens, right? So you've got to find other ways to do it. So empirical data, however nice that would be to have. You can't always have that. So simulations are a really good way of doing that. In some organisations you may be able to run a business simulation or build a model internally, a computational model to model a system and measure the effects of that given variable. See see what the difference is depending on the price of your goods or the cost of raw materials, or whatever it may be, you may have a clear picture of your production line, and maybe it's easy for you to model that. Um, or you could just make an estimate. You can estimate a low, medium and high value. I mean, that seems a pretty crude way of doing it, but it may be useful for some some things. Perhaps simple estimates within a perhaps a more comprehensive model might be a good combination. And there's two things we can produce. When we do sensitivity analysis, we can produce a diagram like this called a spider plot or a spider diagram. And what it shows us here is what the output variable is in this case revenue. How it affects, how it changes depending on deviations from the base value. So in this in this example here for example, I think in fact we're going to talk through it in a second. But it kind of looks like this like spidery. Some variables are positively correlated with with changes and some are negatively correlated. And you can sort of see the steeper slope ones are more sensitive. And we can also produce a tornado diagram which shows the same thing. It shows kind of the the range of values here as a bar. Let me do a little close up and show you what that looks like. Okay. So in this example you're an organisation that does stuff with fuel and sells. You're selling a product which depends on the price of fuel. Maybe the price of fuel affects the cost of delivery or something like that. Um, so you pick a base value. That's probably how much we're currently, how much fuel currently costs or how much the product currently sells for the current unit price. That's what happens if we do nothing. Right? So that's right in the middle of the diagram. That's our current revenue $400,000. That's kind of the base value. Um, and then we calculate the sensitivity curve for a given variable, in this case its unit price. So if we change the unit price of our product this is quite a nice, um, a nice scenario where as we increase the price of our product, we make more revenue. That's handy. Let's just charge more and we'll make more money. I think that probably works to some extent. I think realistically this probably would plateau. I don't think you can just keep charging more and getting more money. Otherwise there's a there's a supply and demand problem in your industry that probably needs to be rectified. But if you did get this idealised curve, that'd be fantastic. You'd probably act on that.

SPEAKER 2
I'm.

SPEAKER 1
But you calculate that sensitivity curve through either calculation or a model or empirical data. Similarly, you can look at the cost of fuel and see what happens as you increase the cost of fuel. If the cost of fuel were to go up by 20%. This is how much our revenue would be if the cost of fuel went down by 20%. This is what would happen. Now you can imagine if your business was so sensitive to the cost of fuel, you'd probably want to switch to a fleet of electric vehicles. Or you'd probably want to manage that risk somehow because otherwise, you know, maybe start stockpiling or storing fuel so that your business wouldn't have to deal with fluctuations in fuel price since you're so sensitive to it. Does this make sense? Hopefully you can read and interpret graphs like this in an exam sort of scenario. So how do we turn this into a tornado diagram? Well, you just take the range of values. So in this case we're deciding the plus or -25% range. So the range of revenues if we were to deviate by plus or -25% and create a bar based on that and stick that in the plot. And similarly for this one.

SPEAKER 2
Okay.

SPEAKER 1
Sometimes you colour the bars depending on the direction as well. So the cost of fuel is negatively correlated. So you might use a different colour for that one. So it's clear that that's what's happening. Here's an example tornado diagram showing that that concept. I won't talk about this particular example. I think it's too complex to talk through in a lecture. And you're not medics. But the idea is that these these ideas of sensitivity analysis is a very common method used in science to try and figure out which variables and which risk risk factors. Sorry, which rich I can't say it. I'm not going to say it. Then which things cause the the thing to more likely happen. Risk factors. That's the word I'm trying to use. Okay. So we've talked we've talked about causes. We've identified the causes. We've analysed those causes we understand which causes we're most sensitive to. Right. So in our risk planning process we now know what's what the problems may be. That's the key thing we've got to now. So what will we do if these risks come to life. That's where planning risk responses comes in. Now most organisations have a risk management plan. Now, this is not a risk plan of what we're going to do for particular risks. This is a risk management plan. This is the plan of how we're going to manage risk as an organisation. So this. This will involve. It's not a plan for when something goes wrong that's wrong. Ignore that slide that slides. Incorrect. A risk management plan is not a specific plan for when something goes wrong. The risk management plan is the plan about how the organisation is going to deal with risk in general. So that slide is very misleading. Expensive. Everyone got my exam question wrong a few years ago. Because my slides. There we go. So there's a risk a risk. We've identified that my slides can't be trusted. If you come to the lectures, you'll be safe from that. Thankfully. Sorry to everyone who's not here, but guess you can't hear that. Sorry. So it's no use to you.

SPEAKER 2
Um.

SPEAKER 1
Right. So you tend to have a general risk response. And the risk response will depend on probability and impact. You may decide as an organisation that if there's a high probability high impact risk, then you'll avoid it if it's very likely to happen. And it's very disastrous to the business. We will avoid those risks. You might decide that low probability, low impact risks will just buy some insurance. We'll transfer those risks. You might decide that if it's high probability but low impact, you will mitigate it. It's going to happen. So we should probably do something about it, but it's not that bad that we're going to have to worry about it. So we'll just maybe try and reduce the impact a little bit. We'll just try and sorry. We'll just try and reduce the chance of it happening perhaps. And likewise if it's low probability but high impact. Low probability. Yeah. Low probability high. In fact I've got this the wrong way around. Never mind let's say low probability low impact. We might accept that one. It's not likely to happen. It's not going to cause much damage. We'll just accept it. So there are different ways that we can respond. We don't have to follow this route. So what we tend to do as an organisation is have a thing called a risk matrix, which will define the strategies that an organisation wants to take. So we'll we'll produce something that looks like this where we've got likelihood here. You can see this is a very a very coarse scale. Right. What does very unlikely mean. There's no number associated with that. You may think that's a bad idea. Not having some quantification, but some people would argue that quantification is gives a false sense of precision. If someone said the impact is is 3.7 and the likelihood is 0.12, you might, you know, you might think that's a real probability. You might actually, you know, act on that information in a way that is perhaps not the right thing to do, whereas, you know, it gives a false sense of precision, is the term to use. And so we categorise risks according to probability and impact. And we might have different categories, different plans different strategies depending on on where things sit in here. It's obviously very subjective. It's prone to bias. But what it's trying to do is trying to capture the organisation's risk appetite. And this matrix doesn't have to be symmetrical. You may have an asymmetric matrix about this axis here. Which is something you should consider. I'm not going to talk in detail about that, but it's something that you may need to think about. So you may have heard of the principle, the 8020 rule. What this Perito principle tells us, what this observation tells us is that most things that go wrong happen in a small. There's a small number of things causing most of the things that go wrong. So these are some examples of things that follow the Preta principle. 20% of people in the country or in the world own 80% of the wealth. I think the numbers are actually much harsher than that. The 8020 rule isn't a precise quantification. It's just, you know, the few own, the most 20% of the code has 80% of the bugs. Right. That's a common software Perito principle. It takes 80% of the time to write the hardest 20% of the code. You will get through the first 80% of your dissertation projects in 20% of the time. And it's going to that last bit, that home straight that's going to take the longest. It's going to be the hardest, right? The top 20% of the most reported bugs cause 80% of the related errors and crashes, right? The point being, not that, not the number 80 or 20. That's just, you know, it's going to differ from industry to industry and problem to problem. The point is that you need to prioritise. There's no point dealing with 100% of the risks. You should focus on that 20% of the risks that are causing 80% of the problems. Prioritisation. Taking the lowest hanging fruit is the mission. So one way of doing this is through a method called FMA is stands for Failure Modes effects analysis. And there's a quote here saying it's an inductive reasoning single point of failure analysis. And I think I asked a horrible exam question about five years ago saying, what does that mean? And nobody knew. I knew because I googled it. It's, you know, if I present something on a slide that you don't understand, you should probably look it up, right? And not just assume that every bit of information you need is written on my slides. But what that does mean is that you use forward logic, i.e. induction, to make your decisions. And it's a single point of failure analysis, meaning you only evaluate each failure in isolation. You consider each risk on its own. There's a certain probability that two risks will happen at once, and that might result in a very different analysis of what to do. So it's a very simple idea where we treat each risk in isolation and consider what happens in those situations basically. And how do we do it? Well, we have a table of the failure modes, the things that might go wrong. A table of the in that same table on that row. What the the effect of that risk would be, what the damage caused would be we consider what the possible causes are and how we can detect those things happening. And then we come up with some numbers to quantify whether that's a big problem or not. Right. So the numbers are explained in this blue box here. So we have a number for the severity. That's how bad the thing is if it were to happen. The probability of occurrence. That's how likely it is to have happened. To happen. Sorry. And the we call it here the detection rate de. But that's a bit misleading because actually these numbers are always bigger when things are bad. So actually it's the really the undetectable rate. It should be called in the official definition it's detection rate. And that's why it's a D. But it should really be the undetectable rate. How difficult it is to detect. Does that subtlety make sense that these the numbers that we assign the score from 1 to 10 is a bigger number? If things are bad, a smaller number if things are good. And from those three numbers we can calculate the criticality. That's the severity severity times the occurrence probability and a risk priority number which multiplies that same that criticality by the undetected bility, i.e. a bigger number that we get out of this is if you get a really big number, it's going to be really severe, it's really likely, and we're going to be really difficult to detect. It's happened. So let's look at an example. Right. If you are developing software for an ATM. The Pin authentication module needs to make sure that it doesn't give money out to the wrong person, or give the money if they're pins wrong. Deals with stolen cards and things like that, right? So this is a pretty important part of a system. There are two potential failure modes. There are probably hundreds, but there are two that have been mentioned here unauthorised access and authentication failure. So what we can do is think about what. What the impact of those things would be on the customer. If someone got unauthorised access to someone's card and money, they could take money. The customer would be very dissatisfied. If authentication failure happened. If the person's Pin was typed in and it didn't work, they'd be pretty annoyed because they couldn't get their money. One of these is clearly much worse than the other, but perhaps it's less likely. So we need to start quantifying things. So how severe is this top one? What? It's really severe. We do not want someone getting access to your money. You'd much rather your money was still in your bank than in someone else's wallet, right? So the severity is much higher in the top one potential cause as well you can there may be many causes and they may have different probabilities, but a lost and stolen ATM card is the one considered for the top. If you lose your card, there's a chance of unauthorised access to your system, whereas maybe it's the networks down and it can't verify your card in the second situation. So maybe this is more likely than this. Maybe people losing their card is very unlikely. What do we currently have to solve that problem? You can block the card if there are three failed attempts, or you can deal with an improved load balancer to deal with network issues. What is the chance of detectability? So this one has got a number three. That means it's easy to detect. It's easy to detect if someone's putting in the pin incorrectly. It's easy for the customer potentially to detect that the card has been stolen because they've got a big bruise in their head from where the mugger grabbed it. It's maybe harder to detect. The internet issues connecting causing the ATM failure. So maybe that's got a higher number you'd have to justify in an exam you'd protect to justify which numbers you chose based on the information given in the question. Okay, but the key point to remember here is that this is easy to detect. Hence it's got a small number a small D. So the RPN is just the product of those three. So we've decided that these two risks are probably almost equally weighted. Perhaps this one is slightly worse. And then of course, you can come with some possible recommended actions, things you will do to improve the system. So we've got a couple here. Right. You could set a withdrawal limit that would reduce the severity of the risk. So even if someone did get your card and could start stealing your money, they could only steal so much in a day. So that's a pretty good measure. You could introduce two factor authentication into ATMs. Might increase increase in convenience as it so often does, but it would certainly reduce the chances of someone stealing your money considerably. And so there's a full table there for you to study in your own time. But what I also want to say to you is, I think called extended failure modes effect analysis. It's where you do the same as we just talked about, but you then redo the analysis when considering the different possible improvement ideas. So you extend it by thinking of the possible corrective actions for your highest priority risks. You work out the feasibility of them, you assign an F score. That's how feasible it is to do that idea. And then you pick the corrective action with the most feasible reduction in risk. So this is again very simple. You just assigning a number an F number of feasibility. You re-evaluate S and D values and recalculate the RPN. And then you compute this changing RPN divided by feasibility to work out which is the has the greatest reduction in risk. So let's look again at the same example. Um.

SPEAKER 3
So you'll recall.

SPEAKER 1
I had a couple of ideas for actions recommended. We can recompute the values. So for example the severity was eight for the lost ATM card. If you set a withdrawal limit, the severity has gone down slightly because we've reduced the amount that could be stolen. The two factors of notification. It massively reduces the probability of someone actually being able to get access to your account. If you improve the network connection, you might reduce the occurrence probability. And if you have some kind of improved handshaking protocol, you might be able to detect more easily when the network is down right. These are all pretty simple things. In an exam situation, you should be able to come up with new values for and you can recalculate RPN for those values. Feasibility is something you need to work out. How easy is it to implement this idea? Now of course this is so a smaller number is better. So one means easy. So it's actually the feasibility not the feasibility. Just like the detectability column is the detectability column. Apologies I didn't invent the method. So small numbers means easier just like before. And then you calculate the delta divided by the feasibility to work out which one to do. So in this situation. This one here has the greatest reduction. It will actually this one here. But of the example we had this idea is the best. And we should implement that one first. That's our priority. So this is how we prioritise risk decision making. Okay. In this case, though, it was this one, a much bigger number, but I didn't talk about that one.

SPEAKER 2
Okay. So.

SPEAKER 1
Quick recap then before we have a short break and I'll do a few questions and then we'll move to our guest lecture. So today we've talked about risk in projects. Risk is something that is inevitable in software development. But risk isn't just the negatives. Risk is about finding opportunities and managing risk to to manage to maximise the benefits of those opportunities. It's not just about being afraid of doing things and constraining a project, it's really about seeking those potential benefits. And we talked about RACI matrix, which is where you assign responsibilities. We talked about the importance of identifying risks through some kind of systematic brainstorming technique, to have exhaustive coverage of all possible risks, not just a few that popped into your head, but a really exhaustive amount. We work back from those risks and identify the causes of those risks. And we, you know, scientifically try to evaluate the sensitivity of our organisation to those different risks through the sensitivity method. And then we think about different ways of dealing with risk, prioritising those risks and choosing risk responses. The risk management is relatively easy, but it's something that people so often do badly because it requires this rigour and this systematic interaction with things going on in the project.

SPEAKER 2
Um, now.

SPEAKER 1
I'll move on to questions before we'll have a break. But after the break, we're going to have a guest from Deutsche Bank, Craig Adamson. He's going to tell us about risk in financial projects. Very interesting stuff. He's an expert in risk management and has been doing that for many years. So you should have some really interesting insights from him after the break. But before the break let us have. Some Q&A. I bet there's a lot.

SPEAKER 2
Okay.

SPEAKER 1
Peter assessment due on the 23rd of November. Thanks for the shout out there. That's that may be true. I don't I don't know, the current date sounds about right yet. Thursday you'll need to. Everyone needs to do the peer assessment. You should all be submitting feedback for five peers. You should have access to all of that on Moodle. Has anyone had any issues with that? Is that working okay for everyone? Good. Keep your feedback. Good. We're going to be reading some of the essays and marking the feedback to make sure that everyone sort of engaging with that, but hopefully you're finding that quite eye opening, seeing how bad some of your peers are and how good your essay was compared to them. Or maybe that golden nugget of idea that you can steal for your own, for your own project. I should clarify, I've had a lot of questions about this. You can modify your essay after after the feedback round, you can modify it before the final submission. So this is a real opportunity for you to learn some really good ideas and improve for next time. Are we? How exactly do you evaluate critical analysis in marketing?

SPEAKER 2
Um.

SPEAKER 1
I suggest you read the marks game carefully when you're marking the different criteria. There's a video as well which talks you through what critical analysis means, but the idea of critical evaluation is that you are judging a person's work. So you're judging whether the person has synthesised evidence to form balanced judgements. They need to be balanced in the sense that they are considering both sides of different arguments and making reasoned judgements based on that. So that's what we're looking for in critical analysis. If you're not sure what the term means or you need to see some more examples. Search online. Google it. It's a standard term that's kind of common to the way a lot of your work has been marked thus far, but it's important that you understand what that means. So please do engage with that a bit more or send me an email if you need some more clarification or you're not sure if something is evidence of critical analysis.

SPEAKER 2
Yeah.

SPEAKER 1
Where we are actually expected to evaluate our sources for the essay. If you include references in the essay, you should probably have considered what those references refer to, and you should be sort of including discussion of that evidence in the decisions that you've made. So if you refer to the PMBoK guide, you may want to discuss something about that rather than just sort of name dropping it to pretend that you read that book. Yeah. So there was expected to be some amount of evaluation to get the highest marks. But understandably, you can't evaluate every single reference. You have to sort of pick and choose the ones that you think are relevant to evaluate in order to make the point that you're trying to make. An essay is about making an argument, right? So you need to synthesise evidence and make an argument based on that knowledge. That doesn't mean you evaluate every single source in painstaking detail, but you certainly need to make some evaluations where there is some, perhaps conflict between different sources. Is the guest lecturer examiner. There is content in the guest lecturer that that may be examined. Yes. Never mind. Okay. Are the slides broken for you guys as well? Are the slides broken?

SPEAKER 2
For anyone.

SPEAKER 1
Is anyone got broken slides? I'm not seeing anyone saying so. No, sorry. Just you. Whoever wrote that to you on Thursday, yet that's already been answered by someone else. What will the letter stand for on the charts? Well, sorry for that. Not making that clear in the RACI matrix, I assume you're referring to responsible, accountable, consulted and informed what the letter stand for M. Excellent. And the person at home got that one right. Well done. Okay. So in. At 14 minutes past, we will have our guest lecture. So you have a five minute break. So make the most of that and I'll see you all in five minutes time. Thank you.

SPEAKER 2
All set up now.

SPEAKER 1
I guess once you put that one, everyone can hear what you're saying. So. You got some print out now, just in case there's a system failure.

SPEAKER 2
You can.

SPEAKER 1
Put them on here, a few.

SPEAKER 2
Pieces and things. Right.

SPEAKER 1
So I'm going to run this off. Yes I've got your slides in here so you can use this thing if I just press down I mean I can do it for you if you like. Right down to go between the slides, the slides to, to shoot through, to be honest. Yeah. If you want to have them ask questions and things, we can switch Tab to this one. And they can ask you questions via the live chat okay. As opposed to you want me to do that and then just just go for you because it's got the polls built into your slides as well. And I can step on because with this in too much time on that at the end and stuff, I'll try and take a little bit of. So people will submit might submit this question. I mean you've got to give it time, leave a little bit. You've got to give me a bit of time to do that. So I've mentioned at the start, if you've got any questions, post it in the chat and you can come back to them. That's fine.

SPEAKER 2
Do you.

SPEAKER 1
Need anything? Do you need water? No, no, we're good to be here. Yeah, I can wait. Yeah. It's not in three minutes, so. Yeah. So the aim is to finish at 5 to 5. Okay.

SPEAKER 4
So fine.

SPEAKER 1
But I've got a couple of.

SPEAKER 4
And then I can always just skip stuff.

SPEAKER 1
Yeah. Yeah. So if maybe you tend to look at questions or something I've got a little I've got a slide at the end I think of the whole deck and I'll come to it. What I just jump over. It's just kind of like a.

SPEAKER 4
Summarised.

SPEAKER 1
Summary kind of thing. Yeah. If there's no time for it, there's no big deal.

SPEAKER 4
No. That's fine.

SPEAKER 1
If you finish by ten, keep it to.

SPEAKER 4
Well, maybe you just kind of give me a nod if we can get to quarter two.

SPEAKER 2
Sounds good.

SPEAKER 1
Yeah, I should have got time on this.

SPEAKER 4
Where do I need to put this?

SPEAKER 1
Anywhere in your lapel or in that bit, maybe.

SPEAKER 2
Yeah, that's onto you.

SPEAKER 4
Okay, so it's already switched on it.

SPEAKER 1
They disable the on off button because people kept turn them off and then it not working. Had a few lectures, but it hasn't worked at all. And it's really annoying for the people at home because they can't hear anything I'm saying. Right. So you've got the time in there as well, if you need that. Yeah. Just make sure manage all risks that might not happen during the lecture. I have someone hurling a strawberry at you or something. You don't like colours, you see? Didn't manage.

SPEAKER 2
That risk.

SPEAKER 4
So can I flick through the slides with this as well and also do it through that?

SPEAKER 2
But I'll use that thing and I'll do it for you guys.

SPEAKER 1
You said next time please.

SPEAKER 4
So if. Yeah fine. Okay.

SPEAKER 2
On there now if you like. Yeah.

SPEAKER 1
We start at 14.

SPEAKER 4
Yeah. 14. Is that what you said? Yeah. You did.

UNKNOWN
That's right up there.

SPEAKER 5
Yeah. Feel free to ask them questions and try and interact with them a bit. Yeah, they seem a bit quiet today, but I'll chip in some heckles and things if you like.

SPEAKER 2
Go for it.

SPEAKER 5
Is there a different Nomura in terms of.

SPEAKER 4
Giving him a call to save up for later? Well, I.

SPEAKER 5
Might ask you I might ask you.

SPEAKER 2
That. Yeah.

SPEAKER 4
Prepare response.

SPEAKER 5
I mean, because being a Japanese. I imagine there are differences in philosophy.

SPEAKER 4
When it comes to culturally. Yeah. Quite different. We'll get the same goal, but of course.

SPEAKER 5
What? Would you change the slide? I think I did.

SPEAKER 2
Here we go.

SPEAKER 5
Cool. I'm going to go sit down.

SPEAKER 4
Okay.

SPEAKER 5
I'll get my phone, actually. Then I can.

SPEAKER 4
Also. You go sit down.

UNKNOWN
Do you think? Okay.

SPEAKER 4
Yeah. We good to go. Excellent. Well, first of all, thank you very much for inviting me here today. It's a pleasure to be able to come. I've done this before, and it's always been something I've looked forward to and to be able to have the opportunity to discuss, firstly, my career that I've been doing and also try to bring a bit of life to some of the theories, I guess, and the actual text of books with regards to managing projects, project management, risk management through our financial services. So flicking on to the next slide quickly, I'll just give you a bit of an introduction as to what I'm going to talk about today, but also if you have any questions as we go through, I'm more than happy to be interrupted. So raise your hand or however you do it digitally, whether I'll pick that up, as are we going? I'm not quite sure, but Ian can maybe check that every every now and again. But please feel free to just ask questions as we go. I'd like to tailor this as much as possible to where you're interested in. If I'm delving into too much detail in any area, feel free to stop me as well. So I'll start a little bit with what's my role? What's my career journey been? I studied computer science long, long time ago now, and that has led on to a career in financial services. So I'll talk a little bit about that. And then I'll go into why project management is so important in financial services. It's not something that people will first think of when you think about a job in financial services. But I've had a long and varied career with lots of interesting project work in this particular industry. So I'll go a little bit into what drives the changes that are required, and then I'll talk a little bit about what the items I consider to be key to managing a project, and I'll try to bring that to life a little bit with a particular example that I'll talk through of a project that I've been involved in as part of my career. And then finally I'll talk a little bit about a fairly unloved topic. I must say, in project management, it's risks, issues, assumptions and dependencies. It often gets forgotten, but it is really important to the success of a project. And so I'll try to be slightly more interactive with that particular part of the lecture as well. So firstly my my role. So I started I studied computer science and mathematics at Durham University. Following on from that I had an internship or as part of that I had an internship at Barclays Bank where I went into their risk management department. I then was offered a graduate program to be part of their graduate program following completion of my degree. And then I spent the first part well of my career. And for the first ten years of my career working for Barclays in a variety of roles. But what I really started to get drawn towards and really enjoyed was actually project management, and it started with testing and analysis, doing risk management as well. But I found that, I must say slightly boring, slightly tedious in terms of the debt there, because it was generally doing the same stuff following a process that was decisions to be made, but generally it was the same type of stuff occurring now over and over. And that's where I fell into as part of that. How can we make things better? How do we deliver systems? How do we deliver process changes to essentially make things better for the bank and for the the department that I was working in? And that's when I very much gravitated towards project management and project delivery. Then after ten years at Barclays of doing a variety of projects, I moved on to a Japanese bank called Nomura, where I ended up running their project team. Across the whole of the risk management space. The team of around 70 project professionals ranging from across the globe. We had people in India, across America, in Europe, and of course have been a Japanese bank, people in Japan as well. But that was actually a really great role that I did for close to eight years and managed to walk through a number of financial crises and different things and deliver projects on a very lean and agile basis, which which was very exciting times. And then now for the last year or so, I've been at Deutsche Bank, big German bank, which has footholds all over the globe. But again, it's another place where I'm able to use my knowledge, my skills in delivering change for the bank in a variety of projects. So it's been what I would say, something that came right the way. Back to what I studied at university with mouse and computer science, and being able to then actually utilise all of that knowledge in in my career, which I've really enjoyed so far. But going on to maybe the financial services industry. And the next slide is to why we really have drivers for change in this particular industry and why we have actually there's a lot of project managers and a lot of project resources working within the these big banks. Firstly, as you would expect, banks are there really well there to serve society and ensure that there is liquidity in the market and that we can help the economy grow. But at the same time, we have shareholders and as part of that, we expect that we have profits. And so some of the projects that we work on are really there to driven to try and improve the profit margins. And when I talk about that, some of the projects that we've been involved, I've been involved in, in particular, you would look at new particular products that a bank is trying to roll out. So we're often trying to be the first to market in terms of giving our clients new ways of might be managing their risk. It might be accessing capital markets. And in order to do that, essentially we need to make sure if we're selling a product to a particular client, then there is a way to manage the risk that arises from that particular product. And in doing that, we need to have projects which will essentially manage the system side of things and the process side of things to ensure that's all embedded into the organisation, so that we're not unnecessarily taking risks and just trying to make a fast book. So one is around improving profit margins. The second really interesting one tapping into new technology. So over the last five years, I would say this is probably been one of the biggest drivers of the change that we're doing, and it's likely to be for the next five years. So one of the major projects that I was delivering at Nomura was around our migration to cloud based technology. So if I give you an example of the type of information that we have in banks, so as a risk management department, we need to understand what the risk of every transaction that we do is and be able to aggregate that up at a high level so that we can understand at a portfolio level, what is the risk that the bank is exposed to now? There's lots of transactions that banks do. There's loans, then there's trading books or whether we're selling derivatives. I don't know if anyone knows much about derivatives, but ways of hedging your risk or your interest rate risk. With rising interest rates. There's lots of different types of transactions that we sell to, to clients to assist them in managing their risk. But as part of that, we need a way of being able to measure the risk profile generated by that. So if we think about all of the derivative transactions that we have across a bank, most of the banks that I've worked for on any day to day basis, we've probably got north of a million, 2 million transactions that are on the books that are generating risk across all of our clients. So that is a huge amount of data that we've got to store, because we need to store and understand all of the data across those transactions with that as well. In order to understand the risk profile, the future risk profile, we generate scenarios. So in a risk profile we will generate a thousand different scenarios, generally across each transaction at a number of different time points through through the future. And it's generally around 65 time points that we will generate different scenario based metrics for each transaction. So that is a huge amount of data that we generate on a daily basis across all. And then we have to use that to generate a risk profile. So with that cloud tech coming back to my original point, cloud technology in the public cloud that essentially is giving us and whether it be Amazon Web Services or Google Cloud, it's giving banks the way to store their data much more cheaply, much with much better analytics tools that the banks can't generate themselves. And it's a huge, huge driver for change in most of the banks that you will see at the moment. The other side, another example would be artificial intelligence. So it's in the news everywhere, everywhere. So is it. Banks have been focusing on this a lot as well in terms of both from the from the profit margin point of view. So how do we do transactions more smartly? How do we develop better analytics for our customers. And that's one big area that is AI is being used. The other side, if I think about projects that I'm involved in with risk management, is how do we use data to essentially be smarter and make better risk decisions? And. Rather than relying on individuals, people who will be following a process, looking at data and then making a decision. Generally a tick box type exercise, we believe we can make better decisions based on the use of AI and utilising all of the data that is across the market that has previously been untapped into. So that's another big area that is being focussed on at the moment, and then changes to business strategy and outside influencers. Well, I mean, I've got a picture on here of kind of the European Union and Brexit and pieces. This was put in a few years back when that was probably one of the biggest drivers of projects with what banks were trying to do. We couldn't sell into the European Union. We had to set up officers in European destinations that encountered all sorts of legal challenges. There was projects, there was system challenges that we had had to do, and that was something political, which influenced a huge amount of spend and capital expenditure in the financial industry. I'd say a much more relevant one now is our ESG. So environmental social strategy, climate risk, for example, whereby banks are very much focussed on how they become green. And not only that, but what are the transition risks of all of their clients? So if I talk about transition risks, you're talking about the fact that clients, whatever industry they're in, potentially are exposed to a transition to a greener climate. So oil companies is a great example. What are their profit margins going to look like in the future if we're if banks have been dealing with them in the past, what is the appetite to deal with them in the future? We also then have physical risks that we need to think about. So with the changing climate that we have, if we've got clients which have warehouses, in particular flood plains, which maybe is something that's emerging now, then where would that leave the bank and exposed in terms of its risk profile as well? So all of that, we're having to utilise data and systems to start to manage those particular risks. And that leads to projects. And the scale of business and systems, I would say, is most banks that I've worked for, all banks that I've worked for, it's only three. But most banks that I'm aware of have generally grown both organically and through acquisitions and through mergers. Now, what's happened over the years is that there's multiple banks with multiple systems that are all trying to play nice together, and that just results in very complex I.T architectures. And that requires a huge amount of project resource to essentially maintain and keep those systems up to date. And then finally the compliance with regulations. When I joined the industry in 2006, shortly after that, there was a credit crisis in 2008, 2009. That has led to the last 15 years of a huge amount of increased regulations by all of the different regulators across the globe, and that has focussed on how do we ensure that we don't get into a situation that the banks got into back in 2009, but that has led to huge amounts of project work over the last 15 years or so to really deal with those new regulations. And there is no let up. It's constantly a changing environment, a really interesting environment as well, because actually what we're trying to do here is ensure that whilst banks will continue to make money, are they safe and that we don't have another financial crisis like we did back then? So that's, that's some of the drivers for for change. And I just keep an eye on time. I think we're doing okay. And what I really wanted to go into now was maybe and just check there's no questions as we've gone through. Should we. Know, kind of the key things that I consider when planning and delivering a project. So these are very generic things. But what I'll do, I'll walk through each of these with a particular example of a project that I've worked on in the past. So let me just set a little bit of context first. So in credit risk when you managing credit risk for a bank, you're looking at the clients and you're looking at what is the probability of that client defaulting. And if so that's your probability of default of the client. Second, if that client defaults what's your exposure to it? How much money are you going to lose from that client defaulting. What I'm going to focus on here is that exposure part. How exposed are you to that client and how much money you're going to lose? So the way that banks manage that is through setting limits. Now it can be as simple as if you're you've got a loan to a client to a big corporation, you make a loan, your limit could be 50 million. And that's the loan that you're, you're giving to the client. So you set a limit of 50 million. That's what you're comfortable with that you would lose. But given the probability of default is quite low on the client, you're probably comfortable with with something that high. But there's lots of products. There's lots of products that we need to manage. And I talked about derivatives earlier. And you need to set similarly limits against laws to limit the amount of exposure that you're going to have in aggregate against your client. Now as part of that. Historically, banks have done that very much through people checking. So a request will come in from whoever it is, the client relationship manager. They will say, my client wants to do this, we need a new loan or we need to hedge our foreign exchange exposure. And as part of that you will then go through there was a whole process gathering information. What legal information is required, looking at the risk profile of the client and then going through various checks within credit risk. Department of credit of risk managers to determine whether they're happy to do that. That's quite expensive, and it's quite time consuming because it takes time for people to look at it to work through. By that point, your client is probably dealing with other banks as well. There's a competitive element that you may be missing here, because if you're not quick to give that approval, they may go off and get it elsewhere. Or you may make the wrong decision because you may feel pressured into making a quick decision. So the project that I'd like to talk through here was called a rules based allocation project. And what we have done through this project is look at automated ways of granting those approvals through utilising data and utilising rules to determine for certain types of products and certain types of clients, whether that can be automatically approved through system checks. So when it comes to the project and we set up that project, there's a number of key things that I you have to think about. Firstly what are the objectives. So with your sponsor with your the project sponsor, what are we actually trying to achieve here. And the first one is well, I guess there's a couple of things that we worked out that we'd like to achieve. One is the actual speed and execution of the decision we needed to quicken that. Or rather than taking maybe three days because you're waiting for someone, it got submitted by the US, someone in the US, and you're waiting for someone in London to come in the next day and then review it, and then with someone else to get further information. It took it could take 3 or 4 days to get an approval for that. So one is speed of execution, two is around actual potential cost saves. So you've got people sitting there who are doing this as a job, walking through, doing the process. It costs money to have people employed doing to that particular, that particular job. So we looked at those as the two main objectives. Then you determine the scope. So sitting down all right. What's the scope of this. Which products do we think could be relevant for this. Where have we got the most deal flow and where do we need to be quick and where can we utilise a computer to to essentially and data to essentially make a decision instead of a person. So we looked at the products, we looked at the legal jurisdictions that we'd be able to do it, and we looked at the kind of types of clients is it generally with other banks? Is it generally with big corporates and institutions? Is it with high net worth individuals, that type of thing. And then we looked at how are we going to deliver this. What's the approach in governance. Well anything like this it tends to require fairly senior authorisation to to make this type of change. But also given banks are fairly heavily regulated, it required approval by the regulators as well. So we had to factor that into our timeframes and into the project approach, ensuring we had all of the relevant governance in place. Then the other key thing benefit milestones. So what are the key were the key project outputs that we were looking at that would create the benefit. So essentially we needed to map back and make sure it was worthwhile before setting out that project, how many transactions would we need to be approving and what would be the potential profit around those transactions to make a cost save and to be able to? So we had to do a whole lot of analysis around the number of transactions that would need to go through this to make it actually worthwhile. The capital expenditure that we would have and spend on all the new IT systems that we would need to develop as part of it. And then a tough schedule. So what do we need to do in order to. What are the tasks that we need to do to deliver this? There's obviously IT tasks. There is data sourcing. There's process changes in terms of okay who needs to submit the request, what data is required to be submitted as part of that to ensure it can go through the checks. And then I talked about the regulatory approval. When does that come in the project plan. When can we make sure we've got those trucked by the project manager to make sure that we're going to tick all of the boxes that are required to make this a success? And then finally there are risks, issues, dependencies and assumptions. So I talked about that regulatory point. That's a big risk. If the regulators turn around and say, no, you can't do this. We could have spent all of our money and then ultimately not be able to to utilise it. So as part of that, when you think about risks you need to think about those upfront. And we then came up with a plan which was around managing that risk. So we knew we would need to meet with our regulators. We talk them through at a very early stage what our thinking was, why are we proposing this? How does it ensure that we've got the right controls in place so that they don't just think we're kind of reducing controls and could potentially expose ourselves to levels of risk that we wouldn't be comfortable with. So all of that we had to go through in a very periodic stages to make sure that the regulators were comfortable with it as well. And ultimately the project ended up being a success. We delivered the project in around nine months. I think it took from for the IT infrastructure to be fully developed across different areas. And as part of that, we managed to utilise what was people sat there on a day to day basis checking these, taking 3 or 4 days to do this, do these particular activities, they could be retrained and worked on in other areas looking at actual really value added risk analytics as well. So it was a success. We continue to we the project continues in terms of looking at other rules that could also be factored in as part of this, and you've got to constantly stay up to date on these type of things when you're working in projects. So that's, that's that those are hopefully that help to explain some of the kind of key things I think about when I'm looking at projects. And then on the next slide, I just check the times 37. So actually what we've got here, it's fairly self-explanatory. I think Ian talked through a number of these things earlier. I mentioned here on risks that there could be positive or negative. That's a very, very important thing. And I'll come on to a couple of examples of that later on. But I think I've talked enough with some examples of this in my previous words, so maybe we'll just skip on to the next one, which would be some examples of risks, issues, dependencies and assumptions just to test whether or see whether you're able to identify the right ones. So first one, I've kind of taken these from projects that I've worked on in the past and looked at the different types of risks and issues, but what I'd like you to do is just think about this as to whether you can think it's an is there a way of people voting on this is so the first one job logs time is required for the data sourcing work required to roll out the system. Do you think that's a risk, an issue, a dependency or an assumption? So let's see what people have voted for. Looks like a few people just making their mind up.

UNKNOWN
Let's talk about it. Yeah.

SPEAKER 4
So 75% dependency. 8% risk, no issues good and assumption good. So most people who've got dependency absolutely correct. There is a dependency that you would have and you'd need to make sure you manage that through the project. I don't know how many people that 8% is on the risk, and I don't know if it's anyone in the room, but would anyone like to hazard a guess as to or if it is someone here, but they'd like to state why they think it's a risk. Just looking around. I'm not. Am I going to get any volunteers as to why anyone would think it's a risk?

SPEAKER 2
See. I.

SPEAKER 4
Yeah, it could be I think the main. So I think it's a really good spot to to pick this up as a risk. It is a dependency. Absolutely. But to think of this in in a risk as well is a very because what I'm most keen on here is it's a specific person. Joe Bloggs needs to work on this particular piece. It's only one person. What happens if that person is not available? That could be a fairly big issue now. It may be through prioritisation they need to work on. So is there a way that actually this could be a team or a set of individuals that could be utilised? When you're thinking about your project, working through with the management team, why is it just one individual who can do that particular piece of work so that whilst it's absolutely a dependency on that, that to have job logs work on that time for me as a risk manager and an expert and a project manager, the thing I'm thinking of there in terms of what could derail my project success, would be the fact that it's one individual who could be very key to the success of that project. So the next one, new regulations are emerging which may require reprioritization of project resources. Just give it a few moments. So actually this is whilst you're all working away on this one. It's very relevant for the job that I do now. I deal very closely with the European Central Bank on behalf of Deutsche Bank, in terms of new regulations that are coming out. There's constantly changes in the regulations. There's the regulators also audit the big banks as well. And when they spot issues, they will create something called findings which need to be delivered and remediated. It's a big part of the role that I do at the moment in terms of ensuring we remediate the different issues and remediate against gaps in regulations. There's always emerging work and you never know what they're necessarily going to result in in terms of the work. So it's a big part. They can come out and you can have six months to deliver something or remediate, and you have to very much look at that. So. As you close that down, as I say very much, a. So a split with a big split between risk and issue. Anyone like to say why they think it's an issue? Let me share them. I don't think it's an issue yet. I can see this is probably going to lead to an issue, but it's a risk at this moment in time. And that's they're emerging and they may require we don't. So we've got to plan for this. We've got to think about this. If you're working on a project and there's new regulations coming through, you might need to think your blogs might need to go off and work on one of the other projects. For these emerging. It's about prioritisation. So you want to be ready to be able to prioritise and have your stakeholders, the people who can make the decisions, aware of the emergence of new items. But it's not at this stage an issue, but it likely will become an issue if you get to the point where you have to prioritise something else over the project you're working on. And that happens a lot. So third one, the lead developer for the project has resigned from the bank. I certainly hope it's not your blogs, but let's have a look. Let's see what people think on this one. And then maybe we'll do one more, just so that I can give the opportunity for anyone to ask me any questions before we wrap up. That's your stop up there, then. So what have we got? An issue. And absolutely I think that one was a fairly obvious, obvious one, but I would hope as well, from what I talked about earlier as well, whilst it's it's most likely an issue now, you could have mitigated this risk. So if it was Joe Bloggs for example, and you'd been thinking of this at the time, that there is a risk around that lead developer leaving the bank or moving on to another project, you can have an action plan in place for should that happen, which would help to mitigate the potential impact of that issue arising. Now, I talk about these because, as I said at the very start of my lecture, it's an unloved part of project management. I find people focus on tasks, people focus on what we need to deliver the benefits. I think the two big parts, if you were to go into project management in the future, that you should always pay attention to one your risks and issues, because that will ultimately allow you to really focus on the potential things. Changes always happen. A project will never run like you set it out from the start. You won't come up with a project plan and it will go smoothly right the way through. There'll be something that emerges. There'll be something that you didn't spot at the start. So you need to be thinking about the risks and issues from a very early point of view, to try and mitigate those, to give yourself the best chance of success. And the other part, that which is not related to risk and issues, but forcing focusing on your benefits and objectives. I think one of the biggest things that you can do for if you're working in an institution, you need to always stop and assess to make sure, and I will come to you in a moment. You always need to stop and assess to make sure that what you're doing is going to deliver business value. It's really important. You shouldn't just deliver something because you've started. You shouldn't if it's if the world changes and you need to revisit. So I take this rules based allocation approach that we looked at before, one of the items. As we look through that, we we had to change approaches. We were going through it because the scope that we set out at the very beginning wouldn't give us all of the transactions relevant to allow us to make the cost saves that we'd identified at the start. So we had to look at that more imaginatively. We needed to look at extra rules that we could potentially bring in, which would increase the risk. But then we had to manage and agree whether that was sufficient for the delivery. So I think it's hopefully that's giving you a bit of a flavour for it. I'll come to your question now.

UNKNOWN
I. I. Can I put that up? This is. Like.

SPEAKER 2
It's about. It is already.

SPEAKER 4
Correct. So yes, in summary, the difference between a risk and an issue. A risk is forward looking. It's the potential of something happening. And you want to think about that now on a forward looking basis, that if this does actually crystallise into an issue, what would you do about it and how can you potentially stop that from developing into an issue? So a risk, forward looking something that may happen, an issue it's happened. What do we do about it now? Hopefully by managing your risks you've reduced the amount of issues. Or if those issues occur, you've got an action plan in place to actually deal with it on a better basis. So they're very intrinsically linked. Now you're kind of an issue that you hadn't identified a risk for. It might just happen that that can be the case. Things things can can occur that you hadn't had the opportunity to. But I think the focus on the risks will hopefully present prevent a lot of them from the starting point. Yes. What regulations? So two big regulations that I'm working on, which and again, regulations tend to be very much focussed on risk. How you manage the risk, because that's much more in terms of the potential downside of a bank if they're to lose money. I would say there's something called CRR three, which is a capital regulatory requirements. And the third version of those, which is essentially changing a huge the methodology with which we generate our exposures on transactions. Now you utilise that exposure that is generated to essentially determine how much capital the bank should hold. So banks have to hold a huge wad of cash on deposit for a rainy day based on the risk profile to all of their clients. So we this CRR three regulation is changing that methodology, which is a big project in itself, but it could dramatically increase the amount of capital that the bank needs to hold. So it's not just a Deutsche Bank, it's to all banks. And depending on how you've been calculating your capital in the past or now, it could be a bigger change. And it could also have a much bigger impact depending on your risk profile. So that's that's one the other area, which I wouldn't say it, but it's really interesting for all banks is around climate risk. It's heavily regulated and governed. I think banks are probably behind the time or the regulators are actually behind the times a little bit in terms of this and have come to it quite late in the party. But we've there's certainly a big focus on how, I would say banks are managing the transition to a greener, greener climate. Okay. I won't go through any more. Maybe we just see. Is that not? Any other questions in the room before I. Hunt back to in. Yes. Who did. Sorry. Oh, yes.

SPEAKER 2
Where do you think they went from?

SPEAKER 4
That's a. That's an interesting question, and I was quite surprised to see that, because it's not the the club that I would have expected to have necessarily come against the foul player. I must admit, I'm not close enough to that. I can only say they've made a bad, bad decision. And ultimately, have they been managing the risk that they would fall foul of those financial fair Play roll rules? I don't know, maybe they have. Maybe they've thought that actually we're likely to get away with this. Or maybe they thought they were within the rules and they didn't believe there was a risk there. But but ultimately, yeah, it's not worked out well for them in terms of that, that process. But maybe they were maybe they were aware of the risk and just decided to accept that because they didn't think anything would come of it. Or maybe they just decided that that there was no risk to them. Baby. Thank you for the question. Yes.

UNKNOWN
Uh uh, we don't have a lot of.

SPEAKER 4
Budget for risk management in a bank. Yes. So there's a risk.

UNKNOWN
I just have all the.

SPEAKER 2
Risks that could have. How much money? Deal with this problem. As.

SPEAKER 4
So there's two. Two. Slightly different. Areas to that. One is banks have a department, a risk management department, which will essentially look to manage the risk and minimise the risks that the bank is the and work to a risk appetite. The bank will have an appetite of how much risk it wants to take, and the risk management department will be budgeted for. It's people like me who are paid for to work for that department, and they will. Ultimately, there'll be a number as to how much it costs to have a risk management. The other part, which I think you were probably referring to more, is what I talked to around capital, which is. Ultimately, banks don't want to lose money, but there's no reward without risk. So you have to be comfortable that there's going to be some risk involved. How to work that out. So there's the risk appetite part, which is saying we have appetite to take on this amount of risk. But the more risk you take on, the more capital you need to hold to cover yourself. Should there be losses in the future so that you don't have to go for government support and get bailed out like happened in the past? That is a very, very complex area of regulations. But yes, banks will ultimately have a budget in terms of how much capital they're willing to hold. And that ultimately plays into what deals you can do with which clients, because ultimately, the more risky deals you do, the more capital you're going to hold. And therefore, if you get to a ceiling of the amount of capital that you can hold, you're going to have to manage your risk much tighter. So answer your question. Yes.

SPEAKER 2
No I don't.

SPEAKER 4
The kind is separate, to be honest. Or in fact. Because ultimately if banks. Raise interest rates. They will also have because they borrow from other banks as well, in terms of the way that they fund things. It's very unlikely to be linked in such way. I think there was a question of pay. And then I'll come to you unless you need to get me off stage. One minute. Yes. Sorry. Say that again. How does it use AI at the moment? These are projects that we're kicking off, but ultimately I think it would be in looking at historical data. So from a risk management perspective, I talked about the two things. One is the probability of a client defaulting, and two is based on that. How much exposure are you willing to take? The probability of default is is probably the main area that we've been focusing on, which is how do you predict if a client is going to default or what that probability is? I and I talk about how I'm talking about utilising data, data, historical data around the number of defaults in particular industries for particular clients, utilising that data, bringing it into the bank over the last 20 years, through different periods, through difficult, different economic environments, is the way that we can translate that data to help with our modelling around the probability of default. And that's one of the main areas we're looking at at the moment to help us make decisions ultimately. Sorry. Last question then.

SPEAKER 6
How do you balance and manage the risk of technology failing, like going down and managing the risk?

SPEAKER 4
That's a it's a it's a very good question. I wish I had half an hour to talk about it because it's very, very relevant but I don't. So and yes, I mean, there is there has been a lot of focus on that in terms of numerous utilising when I, when I was there having backups on prem, so on premise with to ensure that if there was problems or there are problems in the future, we have a backup as well, but also having SLAs or service level agreements with the providers to ensure that we backed up. And sorry, the second part of the question.

SPEAKER 6
How do you balance the.

SPEAKER 4
Which some people do. Yes. And exactly as you utilise more AI, more decision making through the. You've got to make sure you've got the right controls in place around that and the testing of the technology, all of that. We we have seen as we ruled. I mentioned we needed regulatory approval for this rules based application. We have to provide all of the test evidence, all of that to the regulators as part of an audit before we were able to go live to ensure that success. And sorry, I've took up far too much of your time, but thank you very much. Drop the mic. Well.

SPEAKER 2
That's okay. We'll just have. It. Thank you.

SPEAKER 5
I think. I think it's a bright bunch.

SPEAKER 2
Yeah.

SPEAKER 5
You know, really good question.

SPEAKER 2
A long one.

SPEAKER 4
The answer is very long.

SPEAKER 5
But yeah.

SPEAKER 2
Cool.

SPEAKER 5
If you've got anything that you left on the table.

UNKNOWN
I. All right.
